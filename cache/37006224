{"by":"152334H","descendants":174,"id":37006224,"kids":[37007715,37007163,37006815,37006285,37006549,37007965,37013119,37008226,37008078,37007071,37009623,37006498,37007803,37009480,37007226,37006908,37009341,37007731,37007254,37010752,37012429,37009367,37006874],"score":379,"time":1691185029,"title":"Non-determinism in GPT-4 is caused by Sparse MoE","type":"article","url":"https://152334H.github.io/blog/non-determinism-in-gpt-4/","text":"It’s well-known at this point that GPT-4/GPT-3.5-turbo is non-deterministic, even at temperature=0.0. This is an odd behavior if you’re used to dense decoder-only models, where temp=0 should imply greedy sampling which should imply full determinism, because the logits for the next token should be a pure function of the input sequence & the model weights."}